{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_train/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2351212.26it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 302390.61it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2559124.15it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 131313.44it/s]\n",
      "/Users/anubhav/miniforge3/envs/genomics/lib/python3.8/site-packages/torchvision/datasets/mnist.py:335: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1635217280611/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_val/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2343679.47it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_val/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_val/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 134335.80it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_val/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_val/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2573288.92it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_val/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_val/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 167754.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_val/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist_train', download=True, train=True, transform=transform)\n",
    "\n",
    "valset = datasets.MNIST('mnist_val', download=True, train=False, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size= 784\n",
    "hidden_sizes = [128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiClassCrossEntropy(logits, labels, T):\n",
    "\t# Ld = -1/N * sum(N) sum(C) softmax(label) * log(softmax(logit))\n",
    "\tlabels = Variable(labels.data, requires_grad=False).cuda()\n",
    "\toutputs = torch.log_softmax(logits/T, dim=1)   # compute the log of softmax values\n",
    "\tlabels = torch.softmax(labels/T, dim=1)\n",
    "\t# print('outputs: ', outputs)\n",
    "\t# print('labels: ', labels.shape)\n",
    "\toutputs = torch.sum(outputs * labels, dim=1, keepdim=False)\n",
    "\toutputs = -torch.mean(outputs, dim=0, keepdim=False)\n",
    "\t# print('OUT: ', outputs)\n",
    "\treturn Variable(outputs.data, requires_grad=True).cuda()\n",
    "\n",
    "def kaiming_normal_init(m):\n",
    "\tif isinstance(m, nn.Conv2d):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\telif isinstance(m, nn.Linear):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\t# Hyper Parameters\n",
    "\t\tself.init_lr = 0.01\n",
    "\t\tself.num_epochs = 10\n",
    "\t\tself.batch_size = 64\n",
    "\t\tself.momentum=0.9\n",
    "\n",
    "\t\tself.in_features = hidden_sizes[1]\n",
    "\t\tself.n_classes = 0\n",
    "\t\tself.n_known=0\n",
    "\n",
    "\t\tself.class_map=dict()\n",
    "\n",
    "\t\t# Network architecture\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\t\tself.feature_extractor = nn.Sequential(Linear(input_size, hidden_sizes[0]),\n",
    "                        ReLU(),\n",
    "                        Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                        ReLU())\n",
    "\n",
    "\t\tself.fc= nn.Linear(hidden_sizes[1],self.n_classes)\n",
    "\t\t\n",
    "\t\t# self.feature_extractor = nn.DataParallel(self.feature_extractor) \n",
    "\t\t# self.fc = nn.DataParallel(self.fc)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.feature_extractor(x)\n",
    "\t\tx = x.view(x.size(0), -1)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef increment_classes(self, new_classes):\n",
    "\t\t\"\"\"Add n classes in the final fc layer\"\"\"\n",
    "\t\tn = len(new_classes)\n",
    "\t\tprint('new classes: ', n)\n",
    "\t\t\n",
    "\t\tweight = self.fc.weight.data\n",
    "\n",
    "\t\t\n",
    "\t\tnew_out_features = self.n_known + n\n",
    "\n",
    "\t\tprint('new out features: ', new_out_features)\n",
    "\t\tnew_fc = nn.Linear(self.in_features, new_out_features, bias=False)\n",
    "\t\tself.fc = new_fc\n",
    "\t\t\n",
    "\t\t# kaiming_normal_init(self.fc.weight)\n",
    "\t\tself.fc.weight.data[:self.n_classes] = weight\n",
    "\t\tself.n_classes += n\n",
    "\n",
    "\tdef classify(self, images):\n",
    "\t\t\"\"\"Classify images by softmax\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx: input image batch\n",
    "\t\tReturns:\n",
    "\t\t\tpreds: Tensor of size (batch_size,)\n",
    "\t\t\"\"\"\n",
    "\t\t_, preds = torch.max(torch.softmax(self.forward(images), dim=1), dim=1, keepdim=False)\n",
    "\n",
    "\t\treturn preds\n",
    "\n",
    "\tdef update(self, dataset):\n",
    "\n",
    "\t\tself.compute_means = True\n",
    "\n",
    "\t\t# Save a copy to compute distillation outputs\n",
    "\t\tprev_model = copy.deepcopy(self)\n",
    "\n",
    "\t\tclasses = set()\n",
    "\n",
    "\t\ttrainloader = torch.utils.data.DataLoader(dataset, batch_size=32,shuffle=True, num_workers=1)\n",
    "\t\tfor data, label in trainloader:\n",
    "\t\t\tfor item in label.cpu().detach().numpy():\n",
    "\t\t\t\tclasses.add(item)\n",
    "\n",
    "\t\tprint(classes)\n",
    "\n",
    "\t\t# return\n",
    "\n",
    "\t\tnew_classes = [i for i in classes if i not in self.class_map.keys()]\n",
    "\n",
    "\t\tprint(\"Classes: \", classes)\n",
    "\t\tprint('Known: ', self.n_known)\n",
    "\n",
    "\t\tprint(self.class_map)\n",
    "\n",
    "\t\tif len(new_classes) > 0:\n",
    "\t\t\tself.increment_classes(new_classes)\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tprint(\"Batch Size (for n_classes classes) : \", len(dataset))\n",
    "\t\toptimizer = optim.SGD(self.parameters(), lr=self.init_lr, momentum = self.momentum)\n",
    "\n",
    "\t\tfor epoch in range(self.num_epochs):\n",
    "\t\t\t\n",
    "\t\t\tfor i,(images, labels) in enumerate(trainloader):\n",
    "\t\t\t\t# print(labels.numpy())\n",
    "\t\t\t\tseen_labels = []\n",
    "\t\t\t\timages = Variable(torch.FloatTensor(images))\n",
    "\t\t\t\tseen_labels = torch.LongTensor([self.class_map[label] for label in labels.numpy()])\n",
    "\t\t\t\tlabels = Variable(seen_labels)\n",
    "\t\t\t\t# indices = indices.cuda()\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tlogits = self.forward(images)\n",
    "\t\t\t\tcls_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\t\t\t\tif len(new_classes) > 0:\n",
    "\t\t\t\t\tdist_target = prev_model.forward(images)\n",
    "\t\t\t\t\tlogits_dist = logits[:,:-(len(new_classes))]\n",
    "\t\t\t\t\tdist_loss = MultiClassCrossEntropy(logits_dist, dist_target, 2)\n",
    "\t\t\t\t\tloss = dist_loss+cls_loss\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tloss = cls_loss\n",
    "\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Loading training examples for classes [0 1]\n",
      "<torch.utils.data.dataset.Subset object at 0x10f58b3d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anubhav/miniforge3/envs/genomics/lib/python3.8/site-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/2416716263.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask=torch.tensor((all_train_set.targets) == s)\n",
      "/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/2416716263.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = mask | (torch.tensor(all_train_set.targets) == (s+i))\n",
      "/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/2416716263.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask=torch.tensor(all_test_set.targets) == s\n",
      "/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/2416716263.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = mask | (torch.tensor(all_test_set.targets) == (s+i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "Classes:  {0, 1}\n",
      "Known:  0\n",
      "{}\n",
      "new classes:  2\n",
      "new out features:  2\n",
      "Batch Size (for n_classes classes) :  12665\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/2416716263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# # Update representation via BackProp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# # model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/488562621.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    100\u001b[0m                                 \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                 \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                 \u001b[0;31m# indices = indices.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/st/g0z4v7q50sj7prs3cygcjp6m0000gn/T/ipykernel_22903/488562621.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m                                 \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                 \u001b[0mseen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                 \u001b[0;31m# indices = indices.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "num_task_classes=2\n",
    "total_classes= 10\n",
    "num_iters= total_classes//num_task_classes\n",
    "all_classes = np.arange(total_classes)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\ttransforms.Normalize((0.5,), (0.5,)),\n",
    "\t\t\t\t\t\t\t])\n",
    "all_train_set = datasets.MNIST('mnist_train', download=True, train=True, transform=transform)\n",
    "all_test_set = datasets.MNIST('mnist_val', download=True, train=False, transform=transform)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "for s in range(0, num_iters, num_task_classes):\n",
    "\t# Load Datasets\n",
    "\tprint('Iteration: ', s)\n",
    "\t#print('Algo running: ', args.algo)\n",
    "\tprint(\"Loading training examples for classes\", all_classes[s: s+num_task_classes])\n",
    "\t\n",
    "\tmask=torch.tensor((all_train_set.targets) == s)\n",
    "\tfor i in range(1,num_task_classes):\n",
    "\t\tmask = mask | (torch.tensor(all_train_set.targets) == (s+i))\n",
    "\t\n",
    "\t# print(list(mask.numpy()))\n",
    "\tidx = [i for i in range(len(all_train_set)) if mask[i]]\n",
    "\n",
    "\tcurrent_train_set = torch.utils.data.Subset(all_train_set, idx)\n",
    "\tprint(current_train_set)\n",
    "\t# trainloader = torch.utils.data.DataLoader(current_train_set, batch_size=32,\n",
    "    #                                         shuffle=True, num_workers=1)\n",
    "\t# for data, label in trainloader:\n",
    "\t# \tprint(data.size())\n",
    "\t# \tprint(label)\n",
    "\t# \tbreak\n",
    "\t# break\n",
    "\n",
    "\tmask=torch.tensor(all_test_set.targets) == s\n",
    "\tfor i in range(1,num_task_classes):\n",
    "\t\tmask = mask | (torch.tensor(all_test_set.targets) == (s+i))\n",
    "\n",
    "\t# print(all_test_set)\n",
    "\tcurrent_test_set = torch.utils.data.Subset(all_test_set, mask.nonzero().view(-1))\n",
    "\n",
    "\t# print(current_test_set)\n",
    "\n",
    "\n",
    "\t# # Update representation via BackProp\n",
    "\tmodel.update(current_train_set)\n",
    "\tbreak\n",
    "\t# # model.eval()\n",
    "\n",
    "\t# model.n_known = model.n_classes\n",
    "\t# print (\"model classes : %d, \" % model.n_known)\n",
    "\n",
    "\t# total = 0.0\n",
    "\t# correct = 0.0\n",
    "\t# for images, labels in train_loader:\n",
    "\t# \timages = Variable(images)\n",
    "\t# \tpreds = model.classify(images)\n",
    "\t# \tpreds = [pred for pred in preds.cpu().numpy()]\n",
    "\t# \ttotal += labels.size(0)\n",
    "\t# \tcorrect += (preds == labels.numpy()).sum()\n",
    "\n",
    "\t# # Train Accuracy\n",
    "\t# print ('Train Accuracy : %.2f ,' % (100.0 * correct / total))\n",
    "\n",
    "\t# total = 0.0\n",
    "\t# correct = 0.0\n",
    "\t# for indices, images, labels in test_loader:\n",
    "\t# \timages = Variable(images).cuda()\n",
    "\t# \tpreds = model.classify(images)\n",
    "\t# \tpreds = preds.cpu().numpy()\n",
    "\t# \ttotal += labels.size(0)\n",
    "\t# \tcorrect += (preds == labels.numpy()).sum()\n",
    "\n",
    "\t# # Test Accuracy\n",
    "\t# print ('Test Accuracy : %.2f ,' % (100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.NLLLoss()\n",
    "# images, labels = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(images.shape)\n",
    "# images=images.view(images.shape[0], -1)\n",
    "# print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logps = model(images) \n",
    "# loss = criterion(logps, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logps.shape)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# epochs=2\n",
    "# for e in range(epochs):\n",
    "#     running_loss=0\n",
    "\n",
    "#     for images, labels in trainloader:\n",
    "#         # print(images.shape)\n",
    "#         images= images.view(images.shape[0], -1)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         output= model(images)\n",
    "#         loss= criterion(output, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(testloader))\n",
    "\n",
    "# img = images[0].view(1, 784)\n",
    "# with torch.no_grad():\n",
    "#     logps = model(img)\n",
    "\n",
    "# ps = torch.exp(logps)\n",
    "# probab = list(ps.numpy()[0])\n",
    "# print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "# # view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b5a79dab83a4831680fa621fc6936c7fff2d268d3679ee259110d3832e3964b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
